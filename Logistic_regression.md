La r√©gression logistique est un cas particulier de la famille des Generalized Linear Models (GLM), utilis√©e lorsque la variable cible est binaire.

#### odds (ou cotes)
ratio entre la **probabilit√© qu‚Äôun √©v√©nement se produise** / **probabilit√© qu‚Äôil ne se produise pas** :
|odds/cote|calcul par comptage|
|-|-|
|$\frac{\text{mon √©quipe gagne}} {\text{mon √©quipe perd}}$|<img src="img/classification/logistic_regression/odds.png" width=150> 5/3 = 1,7|



#### probabilit√©
Mesure la chance qu‚Äôun √©v√©nement se produise, sur une √©chelle **de 0 √† 1**
ratio d'un **√©v√©nement** / **ensemble des issues  possibles**
|probabilit√©|calcul par comptage||
|-|-|-|
|$\frac{\text{mon √©quipe gagne}} {\text{mon √©quipe gagne + mon √©quipe perd}}$|<img src="img/classification/logistic_regression/probability.png" width=200> 5/8 = 0.625|1 - probabilit√© (inverse)|
|$\frac{\text{mon √©quipe perd}} {\text{mon √©quipe gagne + mon √©quipe perd}}$|<img src="img/classification/logistic_regression/probability_losing.png" width=200> 3/8 = 0.375|1 - probabilit√© (inverse)|


**Si la probabilit√© d‚Äôun √©v√©nement est $ùëù$**<br>
p = probabilit√© de gagner
alors :
|odds|calcul par probabilit√©|
|-|-|
|$$\text{odds}=\frac{ùëù}{1-p}$$|$$\frac{\frac{5}{8}}{1 - \frac{5}{8}}= \frac{\frac{5}{8}}{\frac{3}{8}}=\frac{5}{3}=1.7$$|

Et inversement, la probabilit√© :<br>
$$p=\frac{odds}{1+odds}$$<br>
<br>‚Äã
une probabilit√© de 0,75 correspond √† des odds de 3 (car 0,75/0,25=3) :<br>

$$p = 0{,}75$$

$$1 - p = 0{,}25$$

$$\text{odds} = \frac{p}{1 - p}
= \frac{0{,}75}{0{,}25}
= 3$$


### logit : log (odds)
#### Probleme
Les **odds** sont asym√©triques

|odds|| | |plage|
|-|-|-|-|-|
|tr√®s favorable|<img src="img/classification/logistic_regression/odds_very_high.png" width=150>|32/3|10.7|entre 1 et $+\infty$|
|favorable|<img src="img/classification/logistic_regression/odds_high.png" width=120>|8/3|2.66|entre 1 et $+\infty$|
|d√©favorable|<img src="img/classification/logistic_regression/odds_low.png" width=70>|1/4|0.25|entre 1 et 0|
|tr√®s d√©favorable|<img src="img/classification/logistic_regression/odds_very_low.png" width=150>|1/32|0.031|entre 1 et 0|


√Ä valeur de probabilit√© √©gale mais oppos√©e, la valeur des petits odds est compress√©e compar√©e aux grands odds :<br>
1/6 = **0.17**<br>
6/1 = **6**<br>

<img src="img/classification/logistic_regression/asymetry.png" width=400>

#### Solution

On transforme l‚Äôaxe des probabilit√©s pour pouvoir utiliser un mod√®le lin√©aire :<br>
Du domaine [0, 1] vers (‚àí‚àû, +‚àû) gr√¢ce au logit :<br>
$\text{logit}(p) = \log\left(\frac{p}{1-p}\right)$


Puis, lors de la pr√©diction, on revient de (‚àí‚àû, +‚àû) vers [0, 1]<br>
gr√¢ce √† la fonction sigmo√Øde (inverse du logit) :<br>
$\sigma(z) = \frac{1}{1 + e^{-z}}$

Logarithme naturel = logarithme n√©p√©rien, c‚Äôest-√†-dire en base e (avec $e‚âà2.718$)

|logit (forme classique statistiques)|fonction sigmo√Øde (inverse)|
|-|-|
|transforme une sismo√Øde en droite|transforme une droite en sigmo√Øde<br>fonction d'activation de la regression logisitique|
|√©tire l'axe y de 0 √† 1 vers ($-\infty$, $+\infty$)|restreint l'axe y √† l'intervalle [0, 1]<br>selon la proximit√© avec la fronti√®re de d√©cision|
||prend une valeur r√©elle et retourne une probabilit√©|
|$$\text{logit(p)}=\log\left( \frac{p}{1 - p} \right)$$|$$\text{p}=\frac{e^\text{log(odds)}}{1 + e^\text{log(odds)}}=\frac{1}{1+e^{-z}}$$|




Cela revient √† centrer sur 0 et normaliser:
|$\text{Probabilit√© p}$|$\text{logit(p)}$|Prediction|
|-|-|-|
|de 0.5 √† 1|de 0 √† $+\infty$|classe 1|
|de 0 √† 0.5|de $-\infty$ √† 0|classe 0|

<img src="img/classification/logistic_regression/log_function.png" width=400>



<img src="../memo_maths/img/log10.png" width=400>




<div align="center">
  <img src="img/classification/logistic_regression/log_reg_01.png" width="400" align="top">
  <img src="img/classification/logistic_regression/log_reg_11.png" width="400" align="top">
</div>

<br>

---
|||
|-|-|
|$log(1)=0$|$\log(0) = -\infty$|
||si on s'approche de 0 par des valeurs positives :<br>  $\lim_{x \to 0^+} \log(x) = -\infty$| 



Pour faire simple :
$log(\frac{1}{0^+}‚Äã)=log(1)‚àílog(0)$

$\log\left(\frac{1}{0^+}\right) = 0 - (-\infty) = +\infty$  

ou plus exactement :  
$$\lim_{x \to 0^+} \log\left(\frac{1}{x}\right) = +\infty$$

---

| ![](img/classification/logistic_regression/log_reg_03.png) | ![](img/logistic_regression/log_reg_04.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![](img/classification/logistic_regression/log_reg_05.png) | ![](img/logistic_regression/log_reg_06.png) |
| ![](img/classification/logistic_regression/log_reg_07.png) | ![](img/logistic_regression/log_reg_08.png) |
| ![](img/classification/logistic_regression/log_reg_09.png) | ![](img/logistic_regression/log_reg_10.png) |
