
La regression logistique et la regression lin√©aire font partie des Generalized Linear Model GLM



#### odds (ou cotes)
ratio entre la **probabilit√© qu‚Äôun √©v√©nement se produise** / **probabilit√© qu‚Äôil ne se produise pas** :
|odds|calcul par comptage|||
|-|-|-|-|
|$\frac{\text{mon √©quipe gagne}} {\text{mon √©quipe perd}}$|<img src="img/machine_learning/logistic_regression/odds.png" width=150>|5/3|1.7|



#### probabilit√©
Mesure la chance qu‚Äôun √©v√©nement se produise, sur une √©chelle **de 0 √† 1**
ratio d'un **√©v√©nement** / **ensemble des issues  possibles**
|probabilit√©|calcul par comptage||||
|-|-|-|-|-|
|$\frac{\text{mon √©quipe gagne}} {\text{mon √©quipe gagne + mon √©quipe perd}}$|<img src="img/machine_learning/logistic_regression/probability.png" width=200>|5/8|0.625|1 - p (inverse)|
|$\frac{\text{mon √©quipe perd}} {\text{mon √©quipe gagne + mon √©quipe perd}}$|<img src="img/machine_learning/logistic_regression/probability_losing.png" width=200>|3/8|0.375|1 - p (inverse)|


**Si la probabilit√© d‚Äôun √©v√©nement est $ùëù$**<br>p = probabilit√© de gagner
alors :
|odds|calcul par probabilit√©|
|-|-|
|$$\text{odds}=\frac{ùëù}{1-p}$$|$$\frac{\frac{5}{8}}{1 - \frac{5}{8}}= \frac{\frac{5}{8}}{\frac{3}{8}}=\frac{5}{3}=1.7$$|

Et inversement 
$$probabilit√©=\frac{odds}{1+odds}$$
‚Äã
une probabilit√© de 0,75 correspond √† des odds de 3 (car 0,75/0,25=3)


### logit : log (odds)
#### Probleme
Les **odds** sont asym√©triques

|odds|| | |plage|
|-|-|-|-|-|
|tr√®s favorable|<img src="img/machine_learning/logistic_regression/odds_very_high.png" width=150>|32/3|10.7|entre 1 et $+\infty$|
|favorable|<img src="img/machine_learning/logistic_regression/odds_high.png" width=120>|8/3|2.66| |

||| | ||
|-|-|-|-|-|
|d√©favorable|<img src="img/machine_learning/logistic_regression/odds_low.png" width=70>|1/4|0.25|entre 1 et 0|
|tr√®s d√©favorable|<img src="img/machine_learning/logistic_regression/odds_very_low.png" width=150>|1/32|0.031||


√Ä valeur de probabilit√© √©gale mais oppos√©e, la valeur des petits odds est compress√©e compar√©e aux grands odds :
1/6 = **0.17**
6/1 = **6**

<img src="img/machine_learning/logistic_regression/asymetry.png" width=400>

#### Solution
On **transforme l'axe y** de **probabilit√© de  0 √† 1** en **log(odds)**, forme classique du _logit_ en statistiques : $$\text{logit(p)}=\log\left( \frac{p}{1 - p} \right)$$


Cela revient √† centrer sur 0 et normaliser:
|$\text{Probabilit√© p}$|$\text{logit(p)}$ |
|-|-|
|de 0.5 √† 1|de 0 √† $+\infty$|
|de 0 √† 0.5|de $-\infty$ √† 0|

<img src="img/machine_learning/logistic_regression/log_function.png" width=400>



<img src="../memo_maths/img/log10.png" width=400>




<div align="center">
  <img src="img/machine_learning/logistic_regression/log_reg_01.png" width="400" align="top">
  <img src="img/machine_learning/logistic_regression/log_reg_11.png" width="400" align="top">
</div>

<br>

---
|||
|-|-|
|$log(1)=0$|$\log(0) = -\infty$|
||si on s'approche de 0 par des valeurs positives :<br>  $\lim_{x \to 0^+} \log(x) = -\infty$| 



Pour faire simple :
$log(\frac{1}{0^+}‚Äã)=log(1)‚àílog(0)$

$\log\left(\frac{1}{0^+}\right) = 0 - (-\infty) = +\infty$  

ou plus exactement :  
$$\lim_{x \to 0^+} \log\left(\frac{1}{x}\right) = +\infty$$

---

| ![](img/machine_learning/logistic_regression/log_reg_03.png) | ![](img/machine_learning/logistic_regression/log_reg_04.png) |
| ------------------------------------------------------------ | ------------------------------------------------------------ |
| ![](img/machine_learning/logistic_regression/log_reg_05.png) | ![](img/machine_learning/logistic_regression/log_reg_06.png) |
| ![](img/machine_learning/logistic_regression/log_reg_07.png) | ![](img/machine_learning/logistic_regression/log_reg_08.png) |
| ![](img/machine_learning/logistic_regression/log_reg_09.png) | ![](img/machine_learning/logistic_regression/log_reg_10.png) |
